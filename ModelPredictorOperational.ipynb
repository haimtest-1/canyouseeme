{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, time, sys\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "import psutil\n",
    "import warnings\n",
    "import pickle, datetime\n",
    "import collections\n",
    "import re, hashlib\n",
    "warnings.filterwarnings('ignore')\n",
    "from google.cloud import storage\n",
    "import matplotlib.pyplot as plt\n",
    "import concurrent.futures\n",
    "from itertools import repeat\n",
    "import numpy as np\n",
    "import xlsxwriter\n",
    "from sklearn.model_selection import KFold\n",
    "import gc\n",
    "import inflect\n",
    "import json\n",
    "import gcsfs\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from PyDictionary import PyDictionary\n",
    "\n",
    "def isPersonNoun(x, dictionary):\n",
    "    meanings = dictionary.meaning(x)\n",
    "    if not meanings or not 'Noun' in meanings:\n",
    "        return -1\n",
    "    level = 0\n",
    "    for meaning in meanings['Noun']:\n",
    "        meaning = meaning.lower()\n",
    "        person_definition = ['a human being','a person','someone','somebody','person', 'an individual','a worker']\n",
    "        for definition in person_definition:\n",
    "            if meaning.startswith(definition):\n",
    "                level = 1\n",
    "                break\n",
    "        if level == 1:\n",
    "            break\n",
    "\n",
    "        words = meaning.split(' ')\n",
    "        if words[0] == 'a':\n",
    "            if len(words) > 1 and words[1].find('person') > -1:\n",
    "                level = 2\n",
    "            if len(words) > 2 and words[2].find('person') > -1:\n",
    "                level = 2\n",
    "\n",
    "        if level == 0:\n",
    "            if meaning.find('person') > -1:\n",
    "                level = 3\n",
    "\n",
    "    return level\n",
    "                \n",
    "    \n",
    "def isPersonNounClass(x, person_nouns, non_person_nouns, cache, dictionary):\n",
    "    detection_noun = ''\n",
    "    level = -1\n",
    "    spl = splitUpper(x)\n",
    "    for t in spl:\n",
    "        if len(t) < 3:\n",
    "            continue\n",
    "        if t in person_nouns:\n",
    "            return [1,t]\n",
    "        \n",
    "    for t in spl:\n",
    "        if len(t) < 3:\n",
    "            continue\n",
    "        if t in non_person_nouns:\n",
    "            level = max(0,level)\n",
    "        else:\n",
    "            if t in cache:\n",
    "                cur_level = cache[t]\n",
    "            else:\n",
    "                cur_level = isPersonNoun(t, dictionary)\n",
    "                cache[t] = cur_level\n",
    "            if cur_level == 1:\n",
    "                return [1, t]\n",
    "            elif cur_level == 2:\n",
    "                level = 2\n",
    "                detection_noun = t\n",
    "            elif cur_level == 3 and level != 2:\n",
    "                level = 3\n",
    "                detection_noun = t\n",
    "            \n",
    "    return [level, detection_noun]\n",
    "\n",
    "\n",
    "\n",
    "def isPersonFields(data, person_pii_kw_list): \n",
    "    level = 0\n",
    "    pii_fields = []\n",
    "    for token in data:\n",
    "        words = set(splitUpper(token))\n",
    "        if not len(words):\n",
    "            continue\n",
    "        if words in person_pii_kw_list:\n",
    "            pii_fields.append(token)\n",
    "            level = 1\n",
    "            \n",
    "            \n",
    "    return [level, ', '.join(pii_fields)]\n",
    "            \n",
    "        \n",
    "def loadPersonPiiDetectionModel(person_pii_filename, top_nouns_filename, cache_filename):\n",
    "    df_person_pii_kw = pd.read_csv(person_pii_filename)\n",
    "    df_person_pii_kw.fillna('', inplace=True)\n",
    "    person_pii_kw = {}\n",
    "    for idx, row in df_person_pii_kw.iterrows():\n",
    "        coupled = [x.lower().strip('\"').strip() for x in row['Coupled'].split(',')]\n",
    "        if coupled[0] == '':\n",
    "            coupled = []\n",
    "        if row['Weights'] == 0 and coupled[0] == '':\n",
    "            continue\n",
    "        person_pii_kw[row['PII']] = {}\n",
    "        person_pii_kw[row['PII']]['Weights'] = row['Weights']\n",
    "        person_pii_kw[row['PII']]['Coupled'] = coupled\n",
    "        person_pii_kw[row['PII']]['Standalone'] = row['Standalone']\n",
    "\n",
    "    person_pii_kw_list = []\n",
    "    for k,v in person_pii_kw.items():\n",
    "        if v['Standalone'] > 1:\n",
    "            person_pii_kw_list.append(set([k]))\n",
    "        for w in v['Coupled']:\n",
    "            person_pii_kw_list.append(set([k, w.lower()]))\n",
    "            \n",
    "            \n",
    "    top_nouns = pd.read_csv(top_nouns_filename)\n",
    "    person_nouns = set(top_nouns[(top_nouns['PersonDetectionLevel']==1) & (top_nouns['Dual']==0)]['Word'])\n",
    "    non_person_nouns = set(top_nouns[(top_nouns['PersonDetectionLevel']!=1) | (top_nouns['Dual']==1)]['Word'])\n",
    "\n",
    "    cache = {}\n",
    "    try:\n",
    "        cache = pd.read_csv(cache_filename)[['Word','Level']]\n",
    "        cache.index = cache.Word\n",
    "        cache.drop('Word',inplace=True, axis=1)\n",
    "        cache = cache.to_dict()['Level']\n",
    "    except:\n",
    "        pass\n",
    "    \n",
    "    return person_pii_kw_list, person_nouns, non_person_nouns, cache\n",
    "\n",
    "def personPiiDetection(df, person_pii_filename, top_nouns_filename, cache_filename):\n",
    "    dm_person_candidates = df[(df['PredictDataModel'] == 1) | (df['DataModelLabel'] > 0)].index\n",
    "    if not len(dm_person_candidates):\n",
    "        return df\n",
    "    dictionary = PyDictionary()\n",
    "    for c in ['ClassPersonDetectionLevel', 'ClassPersonDetectionNoun', 'FieldsPersonDetectionLevel', 'FieldsPersonDetectionNames']:\n",
    "        df[c] = ''\n",
    "    person_pii_kw_list, person_nouns, non_person_nouns, cache = loadPersonPiiDetectionModel(person_pii_filename, top_nouns_filename, cache_filename)\n",
    "    df.loc[dm_person_candidates, 'ClassPersonDetectionLevel'],df.loc[dm_person_candidates, 'ClassPersonDetectionNoun'] = zip(*df.loc[dm_person_candidates, 'ClassName'].apply(lambda x: isPersonNounClass(x, person_nouns, non_person_nouns, cache, dictionary)))\n",
    "    df.loc[dm_person_candidates, 'FieldsPersonDetectionLevel'],df.loc[dm_person_candidates, 'FieldsPersonDetectionNames'] = zip(*df.loc[dm_person_candidates, 'DataFieldNames'].apply(lambda x: isPersonFields(x, person_pii_kw_list)))\n",
    "    \n",
    "    pd.DataFrame(cache.items(),columns=['Word','Level']).to_csv('cache_df.csv',index=False)\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_df(fs, file_path):\n",
    "    with fs.open(file_path,'r') as f:\n",
    "        jtable = json.load(f)\n",
    "    return pd.DataFrame(data=jtable['rows'], columns=jtable['header'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def processDataMP(model_type, bucket_name, ml_features, extra_features, features_to_length, labelers = [], csv_prefix = 'features.csv', filenames = [], limit = 0):\n",
    "    client = storage.Client()\n",
    "    bucket = client.get_bucket(bucket_name)\n",
    "    df = ''\n",
    "    zero_label = 0\n",
    "    error_parsing = 0\n",
    "    error_reading = 0\n",
    "    valid_repo = 0\n",
    "    corrupted_files = 0\n",
    "    count = 0\n",
    "    errors = 0\n",
    "    success = 0\n",
    "    df_count = 0\n",
    "    t = time.time()\n",
    "    cur_t = time.time()\n",
    "    \n",
    "    if limit:\n",
    "        filenames = filenames[:limit]\n",
    "        \n",
    "    df = ''\n",
    "    fs = gcsfs.GCSFileSystem()\n",
    "    args = (filenames, \n",
    "        repeat(fs, len(filenames)),\n",
    "        repeat(bucket_name, len(filenames)),\n",
    "        repeat(features_to_length, len(filenames)),\n",
    "        repeat(labelers, len(filenames)),\n",
    "        repeat(model_type, len(filenames)))\n",
    "    dfs = []\n",
    "    mem_usage = 0\n",
    "    file_number = 0\n",
    "    # Create a pool of processes. By default, one is created for each CPU in your machine.\n",
    "    with concurrent.futures.ProcessPoolExecutor() as executor:\n",
    "        for filename, new_df in zip(filenames, executor.map(processFile, *args)):\n",
    "            if not len(new_df):\n",
    "                errors += 1\n",
    "                continue\n",
    "            mem_usage += new_df.memory_usage(index=True,deep=True).sum()\n",
    "            dfs.append(new_df)\n",
    "            success += 1\n",
    "            \n",
    "            count += 1\n",
    "            if count % 1000 == 0:\n",
    "                process = psutil.Process(os.getpid())\n",
    "                print(process.memory_info().rss  /1024**3, 'GB process memory usage')\n",
    "                print(count, round((time.time()-cur_t)/60,1), 'minutes, total = ', round((time.time()-t)/60,1), 'minutes')\n",
    "                print(file_number, 'Memory usage = :',round(mem_usage/1024**3,1),'GB')\n",
    "                cur_t = time.time()\n",
    "                \n",
    "                mem_usage = 0\n",
    "                \n",
    "                file_number += 1\n",
    "             \n",
    "    df = pd.concat(dfs,axis=0)\n",
    "        \n",
    "    print('Memory usage = :',round(mem_usage/1024**3,1),'GB')\n",
    "    print(len(df),'rows in df')\n",
    "    print(count,'repositories in dataset')\n",
    "    print(success,'successfully processed, ', errors, 'errors')\n",
    "    df.reset_index(drop=True, inplace=True)\n",
    "    df.fillna(0,inplace=True)\n",
    "\n",
    "    if len(labelers):\n",
    "        for labeler in labelers:\n",
    "            print(len(df),'classes,', df[labeler[0]].sum(),'classes with positive ',labeler[0],',', round((df[labeler[0]].sum()/len(df))*100.0,2),' % positive labels classes')\n",
    "            for labeler in extractLabelColumns(df, labeler[1]):\n",
    "                print(labeler,': ',int(df[labeler].sum()),'positive '+labeler[0]+' classes')\n",
    "        print()\n",
    "    print('Total = ', round((time.time()-t)/60,1), 'minutes')\n",
    "    return df\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def _extract_callers_of(method_full_name, method_calls_map):\n",
    "    method_full_name = method_full_name.split('(')[0]\n",
    "    return method_calls_map[method_full_name] if method_full_name in method_calls_map else 0\n",
    "\n",
    "        \n",
    "def processFile(filename, fs, bucket_name, features_to_length, labelers, model_type):\n",
    "    try:\n",
    "        new_df = load_df(fs, 'gs://'+bucket_name+'/'+filename)\n",
    "        new_df.fillna(0,inplace=True)\n",
    "    except:\n",
    "        return ''\n",
    "    try:\n",
    "        new_df['Repo'] = new_df['Link'].apply(lambda x: '/'.join(x.strip('https://www.github.com/').strip('http://www.github.com/').split('/')[:2]))\n",
    "    except:\n",
    "        return ''\n",
    "    \n",
    "    for len_feature in features_to_length:\n",
    "        new_df[len_feature+'Len'] = new_df[len_feature].apply(lambda x:len(x))\n",
    "\n",
    "\n",
    "    all_labels_columns = []\n",
    "    try:\n",
    "        for labeler in labelers:\n",
    "            labels_columns = extractLabelColumns(new_df,labeler[1])\n",
    "            if not len(set(labels_columns).intersection(set(new_df.columns))) == len(labels_columns):\n",
    "                print('Error - training dataset must conatin all label columns:',labels_columns)\n",
    "                return ''\n",
    "            all_labels_columns += labels_columns\n",
    "            new_df[labeler[0]] = new_df.apply(lambda row: int(row[labels_columns].astype(int).sum() > 0), axis=1)            \n",
    "    except:\n",
    "        return ''\n",
    "    \n",
    "    if model_type == 'DataModel':\n",
    "        features_df = ''\n",
    "        new_df = detectPiis(new_df, features_df) \n",
    "        \n",
    "    if model_type == 'API':\n",
    "        new_df['InternalClassMethodCalls'].fillna('',inplace=True)\n",
    "        new_df['ExternalClassMethodCalls'].fillna('',inplace=True)\n",
    "        new_df['ClassMethodCalls'] = new_df.apply(lambda row: list(set(row['InternalClassMethodCalls'] + row['ExternalClassMethodCalls'])), axis=1)\n",
    "        \n",
    "        method_calls_histogram = collections.Counter([item for sublist in new_df['ClassMethodCalls'].values for item in sublist])\n",
    "        new_df['MethodCalledLen'] = new_df['MethodUniqueName'].apply(lambda method_name: _extract_callers_of(method_name, method_calls_histogram))\n",
    "    \n",
    "    \n",
    "      \n",
    "    if model_type == 'Test' or model_type == 'TestUtil':\n",
    "        test_classes = set(new_df[new_df[model_type+'Label']==1]['QualifiedName'].values)\n",
    "        new_df[model_type+'InnerClass'] = 0\n",
    "        if len(test_classes):\n",
    "            new_df[model_type+'InnerClass'] = new_df['ContainedByClasses'].apply(lambda x: 1 if len(set(x).intersection(test_classes)) > 0 else 0)\n",
    "            \n",
    "    return new_df\n",
    "\n",
    "\n",
    "def extractLabelColumns(df, label_str):\n",
    "    labelers = []\n",
    "    for c in df.columns:\n",
    "        if c.startswith(label_str):\n",
    "            labelers.append(c)\n",
    "    return labelers\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def detectPiis(df, features_df = ''):\n",
    "    pii_dict = {}\n",
    "    pii_list = []\n",
    "    \n",
    "    df['RawFieldNames PIIs Detection'] = ''\n",
    "    df['PII Level'] = 2\n",
    "        \n",
    "    for idx,row in df.iterrows():\n",
    "        if len(row['DataFieldNames']) == 0:\n",
    "            continue\n",
    "            \n",
    "        if doFilter(row, pii_model):\n",
    "            continue\n",
    "            \n",
    "        cur_features_df = ''\n",
    "        if len(features_df):\n",
    "            cur_features_df = features_df[features_df['ClassName'] == row['ClassName']]\n",
    "            \n",
    "        \n",
    "        fields_piis, fields_piis_matches, fields_num_matches, fields_num_kw_matches, fields_avg_match_len, fields_num_coupled, fields_max_level, fields_scores, fields_piis_coupled, fields_max_similarity_score = findPIIs(row['DataFieldNames'], pii_model, True, cur_features_df)\n",
    "        \n",
    "        class_piis, class_piis_matches, class_num_matches, class_num_kw_matches, class_avg_match_len, class_num_coupled, class_max_level, class_scores, class_piis_coupled, class_max_similarity_score = findPIIs('['+str(row['ClassName'])+']', pii_model, False)\n",
    "        \n",
    "       \n",
    "\n",
    "        words = set(splitUpper(row['ClassName']))\n",
    "        if len(words.intersection(pii_model['pii_class_kw'])) > 0:\n",
    "            df.loc[idx,'PII Level'] = 1\n",
    "                \n",
    "        if fields_num_matches + class_num_matches == 0:\n",
    "            continue\n",
    "            \n",
    "        pii_fields = []\n",
    "        if fields_max_level > 1 or class_max_level > 1:\n",
    "            pii_fields = fields_piis_matches\n",
    "            \n",
    "            \n",
    "        df.loc[idx,'RawFieldNames PIIs Detection'] = ' ;'.join(pii_fields)\n",
    "            \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def doFilter(row, pii_model):\n",
    "    class_words = set(splitUpper(row['ClassName']))\n",
    "    if len(class_words.intersection(pii_model['blacklist']['class_name'])) > 0:\n",
    "        return True\n",
    "    \n",
    "    path_words = set([item for sublist in [splitUpper(p) for p in row['Path'].split('/')] for item in sublist])\n",
    "    \n",
    "    if len(path_words.intersection(pii_model['blacklist']['path'])) > 0:\n",
    "        return True\n",
    "\n",
    "    return False  \n",
    "\n",
    "\n",
    "def filterField(token, features_df):\n",
    "    field_features = features_df[features_df['FieldName'] == token]\n",
    "    if not len(field_features):\n",
    "        return True\n",
    "    try:\n",
    "        if field_features['FieldType'].values[0].lower() == 'boolean' or field_features['IsFinal'].values[0] == 1 or field_features['IsStatic'].values[0] == 1:\n",
    "            return True\n",
    "    except:\n",
    "        print(token,field_features)\n",
    "        if field_features['FieldType'].values[0].lower() == 'boolean' or field_features['IsFinal'].values[0] == 1 or field_features['IsStatic'].values[0] == 1:\n",
    "            return True\n",
    "    return False\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def splitUpper(s):\n",
    "    try:\n",
    "        return [t.replace('_','').replace('.','').lower() for t in re.findall(r'[A-Z_.]?[^A-Z0-9_.\\s]+|[A-Z]+', s)]\n",
    "    except:\n",
    "        print('Error processing: ', s)\n",
    "        return[]\n",
    "\n",
    "\n",
    "\n",
    "def findPIIs(data, pii_model, do_filter, features_df = ''):\n",
    "    piis = []\n",
    "    piis_coupled = []\n",
    "    \n",
    "    num_matches = 0\n",
    "    num_coupled = 0\n",
    "    num_kw_matches = 0\n",
    "    matches = []\n",
    "        \n",
    "    scores = {}\n",
    "        \n",
    "    max_level = 0\n",
    "    max_similarity_score = 0\n",
    "    for token in data:\n",
    "        max_token_level = 0\n",
    "        comb = set([])\n",
    "        spl = splitUpper(token)\n",
    "        if not len(spl):\n",
    "            continue\n",
    "            \n",
    "        if do_filter and spl[0] in pii_model['blacklist']['field_prefix']:\n",
    "            continue\n",
    "            \n",
    "        if len(features_df) and filterField(token, features_df):\n",
    "            continue\n",
    "        for t in spl:\n",
    "            if t in pii_model['pii_keywords'].keys() and pii_model['pii_keywords'][t]['Weights'] > 0:\n",
    "                comb.add(t)\n",
    "                if pii_model['pii_keywords'][t]['Weights'] > max_token_level:\n",
    "                    max_token_level = pii_model['pii_keywords'][t]['Weights']\n",
    "                    \n",
    "                if pii_model['pii_keywords'][t]['Weights'] > max_level:\n",
    "                    max_level = pii_model['pii_keywords'][t]['Weights']\n",
    "                    \n",
    "                \n",
    "        comb = list(comb)\n",
    "        if len(comb) == 0:\n",
    "            continue\n",
    "        scores[token] = {}\n",
    "        scores[token]['similarity_score'] = len(comb) / len(spl)\n",
    "        scores[token]['max_level'] = max_token_level\n",
    "        scores[token]['words'] = comb\n",
    "          \n",
    "        scores[token]['coupled_words'] = []\n",
    "        scores[token]['levels_count'] = {}\n",
    "        for c in comb:\n",
    "            if not pii_model['pii_keywords'][c]['Weights'] in scores[token]['levels_count']:\n",
    "                scores[token]['levels_count'][pii_model['pii_keywords'][c]['Weights']] = 0\n",
    "            scores[token]['levels_count'][pii_model['pii_keywords'][c]['Weights']] += 1\n",
    "            \n",
    "            coupled = pii_model['pii_keywords'][c]['Coupled']\n",
    "            if not len(coupled) or c in scores[token]['coupled_words']:\n",
    "                continue\n",
    "            for couple in coupled:\n",
    "                if couple in comb:\n",
    "                    scores[token]['coupled_words'].append(c+':'+couple)\n",
    "                    scores[token]['max_level'] = 3\n",
    "                    max_level = 3\n",
    "                    num_coupled += 1\n",
    "                    \n",
    "        if scores[token]['similarity_score'] > max_similarity_score:\n",
    "            max_similarity_score = scores[token]['similarity_score']\n",
    "        num_kw_matches += len(comb)\n",
    "        comb.sort()\n",
    "                \n",
    "        piis.append(token+','+str(scores[token]['max_level'])+','+str(scores[token]['similarity_score'])+': '+ ','.join(comb))\n",
    "        if len(scores[token]['coupled_words']):\n",
    "            piis_coupled.append(token+': '+ ','.join(scores[token]['coupled_words']))\n",
    "        num_matches += 1\n",
    "        matches.append(token)\n",
    "    avg_match_len = 0 if num_matches == 0 else num_kw_matches / num_matches\n",
    "    return piis, matches, num_matches, num_kw_matches, avg_match_len, num_coupled, max_level, scores, piis_coupled, max_similarity_score\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def buildModels(df, features, labelers, DT = False):\n",
    "    \n",
    "    models = {}\n",
    "    for l in labelers:\n",
    "        label_col = l[0]\n",
    "        if not label_col in df:\n",
    "            print('Error - training dataset must contain ',label_col,' feature')\n",
    "            return ''\n",
    "        \n",
    "        models[label_col] = {}\n",
    "        models[label_col]['Prefix'] = l[1]\n",
    "        if DT:\n",
    "            clf = DecisionTreeClassifier()\n",
    "        else:\n",
    "            clf = RandomForestClassifier(n_estimators=30, max_features=None)\n",
    "        models[label_col]['clf'] = clf.fit(df[features], df[label_col])\n",
    "    return models\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(df, features, models, logfile = ''):\n",
    "    for label_col, model_data in models.items():\n",
    "        df['Predict'+label_col] = model_data['clf'].predict(df[features])\n",
    "        df['PredictPositiveProbability'+label_col] = model_data['clf'].predict_proba(df[features])[:,-1]\n",
    "        calcPerformance(df, [(label_col,model_data['Prefix'])], logfile)\n",
    "        if 'PositiveMD5' in model_data and len(model_data['PositiveMD5']) > 0:\n",
    "            df['PredictPositiveByMD5'+label_col] = df.apply(lambda row: 1 if (hashlib.md5(str(row[features].astype(float).values).encode('utf-8')).hexdigest() in model_data['PositiveMD5']) else 0, axis=1)\n",
    "            df['MLPredict'+label_col] = df['Predict'+label_col]\n",
    "            df['MLPredictPositiveProbability'+label_col] = df['PredictPositiveProbability'+label_col]\n",
    "            df['Predict'+label_col] = df.apply(lambda row: 1 if row['PredictPositiveByMD5'+label_col] == 1 else row['Predict'+label_col],axis=1)\n",
    "            df['PredictPositiveProbability'+label_col] = df.apply(lambda row: 1 if row['PredictPositiveByMD5'+label_col] == 1 else row['PredictPositiveProbability'+label_col], axis=1)\n",
    "            if label_col in df.columns:\n",
    "                calcPerformance(df, [(label_col,model_data['Prefix'])], logfile)\n",
    "        model_name = label_col.replace('Label','')\n",
    "        df['Predict'+model_name] = df['Predict'+label_col]\n",
    "        df['PredictProba'+model_name] = df['PredictPositiveProbability'+label_col]\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calcPerformance(df, labelers, logfile = ''):\n",
    "    try:\n",
    "        for l in labelers:\n",
    "            label_col = l[0]\n",
    "            print(label_col)\n",
    "            if not label_col in df or not 'Predict'+label_col in df:\n",
    "                print('Error - dataset must contain ',label_col,' and Predict',label_col,'features')\n",
    "                return\n",
    "\n",
    "            tp = sum((df['Predict'+label_col] == 1) & (df[label_col]==1))\n",
    "            fp = sum((df['Predict'+label_col] == 1) & (df[label_col]==0))\n",
    "            tn = sum((df['Predict'+label_col] == 0) & (df[label_col]==0))\n",
    "            fn = sum((df['Predict'+label_col] == 0) & (df[label_col]==1))\n",
    "\n",
    "\n",
    "\n",
    "            print('TP = ',round(100 * tp / sum(df[label_col]==1),1))\n",
    "            print('FP = ',round(100 * fp / sum(df[label_col]==0),1))\n",
    "            print('TN = ',round(100 * tn / sum(df[label_col]==0),1))\n",
    "            print('FN = ',round(100 * fn / sum(df[label_col]==1),1))\n",
    "            print('Accuary = ',round(100 * (tp+tn) /len(df),1))\n",
    "\n",
    "            if len(logfile):\n",
    "                f = open(logfile,'a')\n",
    "                f.write('TP = '+ str(round(100 * tp / sum(df[label_col]==1),1)) + '\\n')\n",
    "                f.write('FP = '+ str(round(100 * fp / sum(df[label_col]==0),1)) + '\\n')\n",
    "                f.write('TN = '+ str(round(100 * tn / sum(df[label_col]==0),1)) + '\\n')\n",
    "                f.write('FN = '+ str(round(100 * fn / sum(df[label_col]==1),1)) + '\\n')\n",
    "                f.write('Accuary = ' + str(round(100 * (tp+tn) /len(df),1)) + '\\n')\n",
    "                f.close()\n",
    "    except:\n",
    "        print('Error calculating calcPerformance')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def splitUpperV2(s):\n",
    "    try:\n",
    "        if not s[0].isupper():\n",
    "            s = s[0].upper() + s[1:]\n",
    "        return [t.replace('-','').replace('_','').replace('.','').lower() for t in re.findall(r'[A-Z_.-](?:[A-Z ]+(?![a-z])|[a-z]*)', s)]\n",
    "    except:\n",
    "        print('Error processing: ', s)\n",
    "        return[]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def hasSerialize(words):\n",
    "    for w in words:\n",
    "        if w.lower().find('serialize') > -1:\n",
    "            return 1\n",
    "    return 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def findDistinctAnnotations(x):\n",
    "    s = set([])\n",
    "    for v in x.values():\n",
    "        s.update(v)\n",
    "    return s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def postProcess(df, model_type):\n",
    "    if model_type == 'API':\n",
    "        df['org_Visibility'] = df['Visibility']\n",
    "        df['Visibility'] = \\\n",
    "            df['Visibility'].apply(lambda x: 0 if x == 0 else 1 if x =='protected' else 2 if x =='private' else 3 if x =='public' else x)\n",
    "        df['ParameterAnnotationsLen'] = df['ParameterAnnotations'].apply(lambda x: len(findDistinctAnnotations(x)))\n",
    "        \n",
    "    if model_type == 'DataModel':\n",
    "        df['HasSerializeFieldAnnotation'] = df['FieldAnnotationsByName'].apply(lambda x: hasSerialize(findDistinctAnnotations(x)))\n",
    "        df['HasSerializeMethodAnnotation'] = df['MethodAnnotations'].apply(lambda x: hasSerialize(x))\n",
    "        df['NonDataFieldNamesLen'] = df['RawFieldNamesLen'] - df['DataFieldNamesLen']\n",
    "        df['FieldAnnotationsByNameLen'] = df['FieldAnnotationsByName'].apply(lambda x: len(findDistinctAnnotations(x)))\n",
    "        df.loc[df['RawMethodNamesLen'] == 0,'LogicMethodRatio'] = 0\n",
    "        \n",
    "    if model_type == 'Test' or model_type == 'TestUtil':\n",
    "        df['PathWords'] = df['Path'].apply(lambda s: str(list(set([item for sublist in [splitUpperV2(x) for x in s.split('/')] for item in sublist]))))\n",
    "        df['ClassNameWords'] = df['ClassName'].apply(lambda x: splitUpperV2(x))\n",
    "        df['ClassAnnotationsWords'] = df['ClassAnnotations']\n",
    "        df['FieldAnnotationsWords'] = df['FieldAnnotationsByName'].apply(lambda x: list(findDistinctAnnotations(x)))\n",
    "        df['RawImportsWords'] = df['RawImports']\n",
    "        df['RawMethodNamesWords'] = df['RawMethodNames']\n",
    "        df['MethodAnnotationsWords'] = df['MethodAnnotations']\n",
    "\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loadPiiModel(filename):\n",
    "    engine = inflect.engine()\n",
    "\n",
    "    df_pii_model = pd.read_csv(filename)\n",
    "    df_pii_model.fillna('', inplace=True)\n",
    "    pii_model = {}\n",
    "    for idx, row in df_pii_model.iterrows():\n",
    "        coupled = [x.lower().strip('\"').strip() for x in row['Coupled'].split(',')]\n",
    "        if row['Weights'] == 0 and coupled[0] == '':\n",
    "            continue\n",
    "        pii_model[row['PII']] = {}\n",
    "        pii_model[row['PII']]['Weights'] = row['Weights']\n",
    "        pii_model[row['PII']]['Coupled'] = coupled\n",
    "        \n",
    "        plural_pii = engine.plural(row['PII'])\n",
    "        pii_model[plural_pii] = {}\n",
    "        pii_model[plural_pii]['Weights'] = row['Weights']\n",
    "        pii_model[plural_pii]['Coupled'] = coupled\n",
    "    return pii_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def finalizeModel(training_df, model_type, train_db):\n",
    "    if model_type == 'DataModel':\n",
    "        pos_df = training_df[training_df[train_db[model_type]['labelers'][0][0]] == 1]\n",
    "        pos_df['signature'] = pos_df.apply(lambda row: hashlib.md5(str(row[train_db[model_type]['ml_features']].astype(float).values).encode('utf-8')).hexdigest(), axis=1)\n",
    "        pos_signatures = set(pos_df['signature'].values)\n",
    "        train_db[model_type]['model'][train_db[model_type]['labelers'][0][0]]['PositiveMD5'] = pos_signatures\n",
    "    return train_db"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def buildTestModelData(training_df, model_type, ml_features, extracted_features_db):\n",
    "    features_names = list(extracted_features_db.keys())\n",
    "    data = []\n",
    "    for idx, row in training_df.iterrows():\n",
    "        my_ml_features = []\n",
    "        for f in features_names:\n",
    "            words = set([w.lower() for w in row[f]])\n",
    "            shared = list(words.intersection(extracted_features_db[f]['words']))\n",
    "            my_ml_features += [f+':'+word for word in shared]\n",
    "            if 'startswith_words' in extracted_features_db[f]:\n",
    "                words = set(['.'.join(w.lower().split('.')[0:2]) for w in row[f]])\n",
    "                shared = words.intersection(extracted_features_db[f]['startswith_words'])\n",
    "                for word in shared:\n",
    "                    my_ml_features += [f+':'+word for word in shared]\n",
    "        data.append([idx,row[model_type+'Label'],list(set(my_ml_features))])\n",
    "    \n",
    "    \n",
    "    ind = [d[0] for d in data]\n",
    "    lab = [d[1] for d in data]\n",
    "    ml_features_map = {}\n",
    "    for i in range(len(ml_features)):\n",
    "        ml_features_map[ml_features[i]] = i\n",
    "    matrix = np.zeros((len(data),len(ml_features)))\n",
    "    for i in range(len(data)):\n",
    "        d = data[i]\n",
    "        for f in d[2]:\n",
    "            matrix[i,ml_features_map[f]] = 1\n",
    "\n",
    "    model_df = pd.DataFrame(matrix, columns=ml_features)\n",
    "    model_df['Ind'] = ind\n",
    "    model_df[model_type+'Label'] = lab\n",
    "    return model_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pii_model_filename = 'PiiModel.csv'\n",
    "\n",
    "pii_class_kw = set([x.strip() for x in open('pii_class_1gram.csv').readlines()])\n",
    "\n",
    "pii_model = {}\n",
    "pii_model['pii_keywords'] = loadPiiModel(pii_model_filename)\n",
    "pii_model['blacklist'] = {}\n",
    "pii_model['blacklist']['field_prefix'] = set(['is', 'has'])\n",
    "pii_model['blacklist']['field_name'] = set([])\n",
    "pii_model['blacklist']['class_name'] = set(['test', 'tests', 'testing','service'])\n",
    "pii_model['blacklist']['path'] = set(['test', 'tests', 'testing'])\n",
    "pii_model['pii_class_kw'] = pii_class_kw"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bucket_name = 'lim-research-features-20'\n",
    "bucket_folder = ''\n",
    "\n",
    "test_phase = '#22'\n",
    "data_model_features_to_length = ['ClassAnnotations',\n",
    "                                 'MethodAnnotations',\n",
    "                                 'DataFieldNames', \n",
    "                                 'RawFieldNames',\n",
    "                                 'RawImports',\n",
    "                                 'RawMethodNames', \n",
    "                                 'RawComments', \n",
    "                                 'StringLiterals',\n",
    "                                 'ImplementedInterfaces',\n",
    "                                 'ExternalClassesMethodCalls', \n",
    "                                 'ExternalCoreMethodCalls']\n",
    "\n",
    "data_model_ml_features = ['IsLeaf',\n",
    "                          'IsSerializable',\n",
    "                          'GetterCount', \n",
    "                          'HasEquals', \n",
    "                          'HasHashCode', \n",
    "                          'IsCloneable', \n",
    "                          'IsComparable',\n",
    "                          'HasToString', \n",
    "                          'LogicMethodCount', \n",
    "                          'SetterCount', \n",
    "                          'AverageMethodBodyLen', \n",
    "                          'MedianMethodBodyLen', \n",
    "                          'StaticMethodsPercentage',\n",
    "                          'PublicMethodsPercentage', \n",
    "                          'ConstructorsCount',\n",
    "                          'HasIdentifier',\n",
    "                          'HasIdentifierAnnotation',\n",
    "                          'LogicMethodRatio',\n",
    "                          'HasSerializeFieldAnnotation',\n",
    "                          'HasSerializeMethodAnnotation',\n",
    "                          'NonDataFieldNamesLen',\n",
    "                          'FieldAnnotationsByNameLen'] + [f + 'Len' for f in data_model_features_to_length]\n",
    "\n",
    "        \n",
    "data_model_labelers = [('DataModelLabel','!ModelLabel!')]\n",
    "\n",
    "api_features_to_length = ['Annotations',\n",
    "                          'ReturnType',\n",
    "                          'ExternalClassMethodCalls',\n",
    "                          'ExternalCoreMethodCalls',\n",
    "                          'InternalClassMethodCalls',\n",
    "                          'ParametersNameToType']\n",
    "\n",
    "api_ml_features = ['MethodCalledLen',\n",
    "                   'BodyLength', \n",
    "                   'HasJavadoc',\n",
    "                   'IsGeneric',\n",
    "                   'IsGetterOrSetter',\n",
    "                   'HttpMethodAnnotation',\n",
    "                   'IsStatic', \n",
    "                   'PostPutAnnotation', \n",
    "                   'RouteLikeAnnotation', \n",
    "                   'EndsWithHttpMethod', \n",
    "                   'StartsWithHttpMethod',\n",
    "                   'Visibility',\n",
    "                   'ParameterAnnotationsLen'] + [f + 'Len' for f in api_features_to_length]\n",
    "\n",
    "        \n",
    "api_labelers = [('ApiLabel','!ApiLabel!')]\n",
    "\n",
    "test_labelers = [('TestLabel','!TestLabel!')]\n",
    "\n",
    "test_util_labelers = [('TestUtilLabel','!TestUtilLabel!')]\n",
    "\n",
    "train_db = {}\n",
    "train_db['DataModel'] = {}\n",
    "train_db['DataModel']['extra_features'] = ['QualifiedName','ExtendedClass','ClassName','Repo','Path', 'Link']\n",
    "train_db['DataModel']['ml_features'] = data_model_ml_features\n",
    "train_db['DataModel']['features_to_length'] = data_model_features_to_length\n",
    "train_db['DataModel']['labelers'] = data_model_labelers\n",
    "train_db['DataModel']['csv_prefix'] = 'class_features.json'\n",
    "train_db['DataModel']['Filenames'] = []\n",
    "\n",
    "train_db['API'] = {}\n",
    "train_db['API']['extra_features'] = ['ClassName', 'ClassQualifiedName','MethodName', 'MethodUniqueName','Repo','Path', 'Link','DeclaringClassFullName','InternalMethodCalls','MethodCalledLen']\n",
    "train_db['API']['ml_features'] = api_ml_features\n",
    "train_db['API']['features_to_length'] = api_features_to_length\n",
    "train_db['API']['labelers'] = api_labelers\n",
    "train_db['API']['csv_prefix'] = 'method_features.json'\n",
    "train_db['API']['Filenames'] = []\n",
    "\n",
    "train_db['Test'] = {}\n",
    "train_db['Test']['extra_features'] = ['ClassName','Repo','Path', 'Link','MethodAnnotations','RawImports','RawMethodNames','ClassAnnotations','FieldAnnotations','ContainedByClasses','QualifiedName','TestInnerClass']\n",
    "train_db['Test']['ml_features'] = ''\n",
    "train_db['Test']['features_to_length'] = ''\n",
    "train_db['Test']['labelers'] = test_labelers\n",
    "train_db['Test']['csv_prefix'] = 'class_features.json'\n",
    "train_db['Test']['Filenames'] = []\n",
    "\n",
    "\n",
    "train_db['TestUtil'] = {}\n",
    "train_db['TestUtil']['extra_features'] = ['ClassName','Repo','Path', 'Link','MethodAnnotations','RawImports','RawMethodNames','ClassAnnotations','FieldAnnotations','ContainedByClasses','QualifiedName','TestInnerClass']\n",
    "train_db['TestUtil']['ml_features'] = ''\n",
    "train_db['TestUtil']['features_to_length'] = ''\n",
    "train_db['TestUtil']['labelers'] = test_util_labelers\n",
    "train_db['TestUtil']['csv_prefix'] = 'class_features.json'\n",
    "train_db['TestUtil']['Filenames'] = []\n",
    "\n",
    "\n",
    "client = storage.Client()\n",
    "bucket = client.get_bucket(bucket_name)\n",
    "for b in bucket.list_blobs():\n",
    "    if len(bucket_folder) and not b.name.startswith(bucket_folder):\n",
    "        continue\n",
    "    if b.name.endswith('method_features.json'):\n",
    "        train_db['API']['Filenames'].append(b.name)\n",
    "    elif b.name.endswith('class_features.json'):\n",
    "        train_db['DataModel']['Filenames'].append(b.name)\n",
    "        train_db['Test']['Filenames'].append(b.name)\n",
    "        train_db['TestUtil']['Filenames'].append(b.name)\n",
    "        \n",
    "for k,v in train_db.items():\n",
    "    print(len(v['Filenames']), k ,'files')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for model_type, model_data in train_db.items():\n",
    "#     if model_type != 'API':\n",
    "#         continue\n",
    "#     print('Training', model_type)\n",
    "#     training_df = processDataMP(model_type, bucket_name, model_data['ml_features'], model_data['extra_features'], model_data['features_to_length'], model_data['labelers'], model_data['csv_prefix'], model_data['Filenames'], \n",
    "#                                 limit = 0)\n",
    "#     training_df = postProcess(training_df, model_type)   \n",
    "#     #training_df.to_csv(model_type+'TrainingData'+test_phase+'.csv', index=False) \n",
    "#     #training_df = pd.read_csv(model_type+'TrainingData'+test_phase+'.csv')\n",
    "#     #print('Training file size: ' ,round(os.path.getsize(model_type+'TrainingData'+test_phase+'.csv')/1024**3,2), 'GB')\n",
    "#     #train_db[model_type]['training_df_file'] = model_type+'TrainingData'+test_phase+'.csv'\n",
    "\n",
    "#     # Build the ML classification model\n",
    "#     #m = 'StartsWithPost'\n",
    "#     #training_df[m] = training_df[m].apply(lambda x: x if not x in ['public', 'private', 'protected'] else 0)\n",
    "#     train_db[model_type]['model'] = buildModels(training_df, train_db[model_type]['ml_features'], train_db[model_type]['labelers'])\n",
    "#     train_db = finalizeModel(training_df, model_type, train_db)\n",
    "#     train_db[model_type]['Filenames'] = []\n",
    "#     pickle.dump(train_db, open('TrainDB_'+test_phase+'.p','wb'))\n",
    "# print('Models file size: ' ,round(os.path.getsize('TrainDB_'+test_phase+'.p')/1024**3,2), 'MB')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_phase = '#22'\n",
    "db = pickle.load(open('TrainDB_'+test_phase+'.p','rb'))\n",
    "\n",
    "# Here you should set the testing buckets names\n",
    "db['API']['bucket'] = 'lim-research-features-20'\n",
    "db['DataModel']['bucket'] = 'lim-research-features-20'\n",
    "db['Test']['bucket'] = 'lim-research-features-20'\n",
    "db['TestUtil']['bucket'] = 'lim-research-features-20'\n",
    "\n",
    "# set to False if csv files don't contain the labelers columns\n",
    "has_labelers = True\n",
    "f = 'log_'+str(datetime.datetime.now()).split('.')[0]+'.txt'\n",
    "open(f,'w').write('Starting prediction: ' + str(datetime.datetime.now()))\n",
    "for model_type, model_data in db.items():\n",
    "    print('Testing', model_type)\n",
    "    open(f,'a').write('Testing'+ model_type+'\\n')\n",
    "    if not 'model' in model_data:\n",
    "        print('No model found - skipping ',model_type)\n",
    "        continue\n",
    "        \n",
    "    client = storage.Client()\n",
    "    bucket = client.get_bucket(model_data['bucket'])\n",
    "    db[model_type]['Filenames'] = []\n",
    "    for b in bucket.list_blobs():\n",
    "        if b.name.endswith(model_data['csv_prefix']):\n",
    "            db[model_type]['Filenames'].append(b.name)\n",
    "\n",
    "    print(len(db[model_type]['Filenames']), ' testing files')\n",
    "\n",
    "    if not has_labelers:\n",
    "        db[model_type]['labelers'] = []\n",
    "        \n",
    "        \n",
    "        \n",
    "    testing_df = processDataMP(model_type, \n",
    "                               db[model_type]['bucket'],  \n",
    "                               db[model_type]['ml_features'], \n",
    "                               db[model_type]['extra_features'], \n",
    "                               db[model_type]['features_to_length'], \n",
    "                               db[model_type]['labelers'], \n",
    "                               db[model_type]['csv_prefix'], \n",
    "                               db[model_type]['Filenames'], \n",
    "                               limit = 0)\n",
    "    testing_df = postProcess(testing_df, model_type)\n",
    "    \n",
    "    if model_type in ['API','DataModel']:\n",
    "        testing_df = predict(testing_df, db[model_type]['ml_features'], db[model_type]['model'],f)\n",
    "        \n",
    "    if model_type == 'DataModel':\n",
    "        testing_df = personPiiDetection(testing_df, 'person_pii.csv', 'top_nouns.csv', 'cache_df.csv')\n",
    "        \n",
    "        \n",
    "        \n",
    "    if model_type in ['Test','TestUtil']:\n",
    "        test_model_df = buildTestModelData(testing_df, model_type, db[model_type]['ml_features'], db[model_type]['extracted_features_db'])\n",
    "        p = predict(test_model_df, db[model_type]['ml_features'], db[model_type]['model'],f)\n",
    "        testing_df['Predict'+model_type] = p['Predict'+model_type+'Label']\n",
    "        testing_df['PredictPositiveProbability'+model_type] = p['PredictPositiveProbability'+model_type+'Label']\n",
    "\n",
    "    testing_df.to_csv(model_type+'TestingData'+test_phase+'.csv', index=False) \n",
    "    print()\n",
    "        \n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
